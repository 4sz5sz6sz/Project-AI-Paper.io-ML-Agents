# Paper.io ML-Agents Colab Training Environment

## ğŸ¯ ê°œìš”

Unity ML-Agents Paper.io í”„ë¡œì íŠ¸ë¥¼ Google Colabì—ì„œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ Pythonìœ¼ë¡œ ì™„ì „íˆ ì¬êµ¬í˜„í•œ í™˜ê²½ì…ë‹ˆë‹¤. ì›ë³¸ Unity ì—ì´ì „íŠ¸(`MyAgent.cs`)ì˜ ê´€ì°° ì‹œìŠ¤í…œê³¼ ë³´ìƒ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë³µì œí•˜ì—¬ ë™ì¼í•œ í•™ìŠµ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Google Colab)

### 1. í™˜ê²½ ì„¤ì •

```python
# Colabì—ì„œ ì €ì¥ì†Œ í´ë¡ 
!git clone https://github.com/4sz5sz6sz/Project-AI-Paper.io-ML-Agents.git
%cd Project-AI-Paper.io-ML-Agents/colab_training

# ì˜ì¡´ì„± ì„¤ì¹˜
!pip install -r requirements.txt

# ì¶”ê°€ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ (í•„ìš”ì‹œ)
!apt-get update
!apt-get install -y python3-opengl xvfb
```

### 2. ê¸°ë³¸ í•™ìŠµ ì‹œì‘

```python
# ê¸°ë³¸ í•™ìŠµ ì‹¤í–‰
!python train_colab.py --steps 100000

# ë˜ëŠ” Jupyter ë…¸íŠ¸ë¶ì—ì„œ
from train_colab import PaperIOTrainer

trainer = PaperIOTrainer(config_path="config.yaml")
trainer.train(total_steps=100000)
```

### 3. ì‹¤ì‹œê°„ ì‹œê°í™”

```python
from visualizer import PaperIOVisualizer
from paper_io_env import PaperIOEnv

env = PaperIOEnv()
visualizer = PaperIOVisualizer(env)

# ê²Œì„ ìƒíƒœ ì‹œê°í™”
env.reset()
visualizer.render_game_state(show_observations=True, target_player=1)
```

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
colab_training/
â”œâ”€â”€ paper_io_env.py       # ë©”ì¸ ê²Œì„ í™˜ê²½ (Unity ë¡œì§ ë³µì œ)
â”œâ”€â”€ train_colab.py        # PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ visualizer.py         # ì‹¤ì‹œê°„ ì‹œê°í™” ë„êµ¬
â”œâ”€â”€ config.yaml          # ML-Agents í˜¸í™˜ ì„¤ì •
â”œâ”€â”€ requirements.txt      # Python ì˜ì¡´ì„±
â””â”€â”€ README_colab.md      # ì´ ë¬¸ì„œ
```

## ğŸ® ê²Œì„ í™˜ê²½ ì„¸ë¶€ì‚¬í•­

### í™˜ê²½ íŒŒë¼ë¯¸í„°
- **ë§µ í¬ê¸°**: 100x100
- **í”Œë ˆì´ì–´ ìˆ˜**: 4ëª…
- **ê´€ì°° ì°¨ì›**: 84ì°¨ì› (Unity MyAgent.csì™€ ë™ì¼)
- **ì•¡ì…˜ ê³µê°„**: 4ê°œ ì´ì‚° ì•¡ì…˜ (ìƒ, ìš°, í•˜, ì¢Œ)
- **ìµœëŒ€ ìŠ¤í…**: 2000 ìŠ¤í…/ì—í”¼ì†Œë“œ

### ê´€ì°° ì‹œìŠ¤í…œ (84ì°¨ì›)
Unity `MyAgent.cs`ì˜ `CollectObservations`ë¥¼ ì •í™•íˆ ë³µì œ:

1. **Ultra Critical 3x3 ê´€ì°°** (45ì°¨ì›)
   - 3x3 ì˜ì—­ì„ 5ë²ˆ ë°˜ë³µ ê´€ì°°
   - ì¦‰ê°ì ì¸ ìœ„í—˜ ê°ì§€ì— ì¤‘ì 

2. **Critical Proximity ê´€ì°°** (9ì°¨ì›)
   - ê·¼ì ‘ 3x3 ì˜ì—­ ìƒì„¸ ë¶„ì„
   - ìƒì¡´ í•µì‹¬ ì •ë³´ ì œê³µ

3. **Immediate Danger ê´€ì°°** (10ì°¨ì›)
   - ì¦‰ì‹œ ìœ„í—˜ ìš”ì†Œ ê°ì§€
   - ë²½, ê¶¤ì  ì¶©ëŒ ìœ„í—˜ë„

4. **Enemy Threat Assessment** (15ì°¨ì›)
   - ì  í”Œë ˆì´ì–´ ìœ„í˜‘ ë¶„ì„
   - ê±°ë¦¬, ê³µê²©ì„±, ì˜í†  ìš°ìœ„ ë“±

5. **Basic Info** (5ì°¨ì›)
   - ìœ„ì¹˜, ë°©í–¥, ì ìˆ˜ ë“± ê¸°ë³¸ ì •ë³´

### ë³´ìƒ ì‹œìŠ¤í…œ
Unity `MyAgent.cs`ì˜ ë³´ìƒ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë³µì œ:

- **ë²½ ì¶©ëŒ**: -80ì 
- **ìê¸° ê¶¤ì  ì¶©ëŒ**: -100ì   
- **ì ì—ê²Œ ì‚¬ë§**: -15ì 
- **ì˜ì—­ í™•ì¥**: +0.1ì  Ã— íšë“ íƒ€ì¼ ìˆ˜
- **ì•ˆì „ì§€ëŒ€ í˜ë„í‹°**: -0.1ì /ìŠ¤í…

### ì•¡ì…˜ ë§ˆìŠ¤í‚¹
- 180ë„ íšŒì „ ê¸ˆì§€ (Unity `WriteDiscreteActionMask` ë³µì œ)
- ë²½/ê¶¤ì  ì¶©ëŒ ë°©ì§€

## ğŸ§  í•™ìŠµ ì„¤ì •

### PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° (Unity ì„¤ì •ê³¼ ë™ì¼)
```yaml
batch_size: 2048
buffer_size: 20480
learning_rate: 3.0e-4
epsilon: 0.2
lambd: 0.95
num_epoch: 3
gamma: 0.99
hidden_units: 512
num_layers: 3
memory_size: 256 (LSTM)
```

### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
- **ì…ë ¥**: 84ì°¨ì› ê´€ì°°
- **ì€ë‹‰ì¸µ**: 512 ìœ ë‹› Ã— 3ì¸µ
- **ë©”ëª¨ë¦¬**: LSTM (256 ì°¨ì›)
- **ì¶œë ¥**: 4ì°¨ì› ì •ì±… + 1ì°¨ì› ê°€ì¹˜í•¨ìˆ˜

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì‹œê°í™”

### 1. TensorBoard ì—°ë™
```python
# í•™ìŠµ ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ TensorBoard ë¡œê·¸ ìƒì„±
trainer = PaperIOTrainer(use_tensorboard=True)
trainer.train(total_steps=500000)

# Colabì—ì„œ TensorBoard ì‹¤í–‰
%load_ext tensorboard
%tensorboard --logdir=runs
```

### 2. ì‹¤ì‹œê°„ ê²Œì„ ìƒíƒœ ì‹œê°í™”
```python
# ê²Œì„ ìƒíƒœ ë Œë”ë§
visualizer.render_game_state(
    show_trails=True,           # ê¶¤ì  í‘œì‹œ
    show_observations=True,     # ê´€ì°°ê°’ ì‹œê°í™”
    target_player=1            # ë¶„ì„ ëŒ€ìƒ í”Œë ˆì´ì–´
)

# ê´€ì°°ê°’ ìƒì„¸ ë¶„ì„
visualizer.analyze_observations(player_id=1)
```

### 3. í•™ìŠµ ì§„í–‰ ëŒ€ì‹œë³´ë“œ
```python
# í•™ìŠµ í†µê³„ ëŒ€ì‹œë³´ë“œ
visualizer.create_training_dashboard(trainer.metrics)
```

### 4. ê²Œì„ ì• ë‹ˆë©”ì´ì…˜
```python
# ê²Œì„ í”Œë ˆì´ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
visualizer.animate_game(
    max_steps=200, 
    save_gif=True, 
    gif_filename='training_demo.gif'
)
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì»¤ìŠ¤í…€ í•™ìŠµ ì„¤ì •
```python
# ì»¤ìŠ¤í…€ configë¡œ í•™ìŠµ
custom_config = {
    'batch_size': 4096,
    'learning_rate': 1e-4,
    'hidden_units': 1024,
    'max_steps': 1000000
}

trainer = PaperIOTrainer()
trainer.config.update(custom_config)
trainer.train()
```

### 2. ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ
```python
# í•™ìŠµ ì¤‘ ìë™ ì €ì¥ (50,000 ìŠ¤í…ë§ˆë‹¤)
trainer.train(total_steps=500000, save_interval=50000)

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
trainer.load_models("checkpoint_100000")
trainer.train(total_steps=200000)  # ì¶”ê°€ í•™ìŠµ
```

### 3. ì—ì´ì „íŠ¸ í‰ê°€
```python
# í›ˆë ¨ëœ ì—ì´ì „íŠ¸ í‰ê°€
trainer.load_models("final_model")
eval_results = trainer.evaluate(num_episodes=50)

# ê²°ê³¼ ë¶„ì„
for player_id, metrics in eval_results.items():
    print(f"Player {player_id} - Win Rate: {metrics['wins']/50*100:.1f}%")
```

### 4. ë©€í‹° ì—ì´ì „íŠ¸ ëŒ€ì „
```python
# ì„œë¡œ ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ì˜ ì—ì´ì „íŠ¸ë“¤ì„ ëŒ€ì „ì‹œí‚¤ê¸°
trainer1 = PaperIOTrainer()
trainer1.load_models("checkpoint_100000")

trainer2 = PaperIOTrainer()  
trainer2.load_models("checkpoint_200000")

# ëŒ€ì „ ë¡œì§ êµ¬í˜„...
```

## ğŸ¯ í•™ìŠµ íŒ

### 1. í•™ìŠµ ì•ˆì •ì„±
- **ë°°ì¹˜ í¬ê¸°**: 2048 ì´ìƒ ê¶Œì¥ (4 í”Œë ˆì´ì–´ ë™ì‹œ í•™ìŠµ)
- **í•™ìŠµë¥ **: 3e-4ì—ì„œ ì‹œì‘, í•„ìš”ì‹œ ê°ì†Œ
- **ì—í”¼ì†Œë“œ ê¸¸ì´**: ë„ˆë¬´ ê¸¸ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ

### 2. ì„±ëŠ¥ ìµœì í™”
```python
# GPU ì‚¬ìš© í™•ì¸
import torch
print(f"CUDA available: {torch.cuda.is_available()}")

# ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ (ë¡œì»¬ì—ì„œ)
trainer = PaperIOTrainer()
trainer.config['num_envs'] = 4  # ë³‘ë ¬ í™˜ê²½ ìˆ˜
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
# ê·¸ë¦¬ë“œ ì„œì¹˜ ì˜ˆì‹œ
learning_rates = [1e-4, 3e-4, 1e-3]
batch_sizes = [1024, 2048, 4096]

for lr in learning_rates:
    for bs in batch_sizes:
        config = {'learning_rate': lr, 'batch_size': bs}
        trainer = PaperIOTrainer()
        trainer.config.update(config)
        trainer.train(total_steps=100000)
        # ê²°ê³¼ ê¸°ë¡...
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
trainer.config['batch_size'] = 1024
trainer.config['buffer_size'] = 10240
```

### 2. í•™ìŠµ ì†ë„ ê°œì„ 
```python
# í™˜ê²½ ë‹¨ìˆœí™” (ë””ë²„ê¹…ìš©)
env = PaperIOEnv()
env.MAX_STEPS = 500  # ì—í”¼ì†Œë“œ ê¸¸ì´ ë‹¨ì¶•
```

### 3. ì‹œê°í™” ë¬¸ì œ (Colab)
```python
# ë°±ì—”ë“œ ì„¤ì •
import matplotlib
matplotlib.use('Agg')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ í•™ìŠµ ì„±ëŠ¥
- **100k ìŠ¤í…**: ê¸°ë³¸ ìƒì¡´ ì „ëµ í•™ìŠµ
- **500k ìŠ¤í…**: ì˜ì—­ í™•ì¥ ì „ëµ ìŠµë“  
- **1M ìŠ¤í…**: ê³ ê¸‰ ëŒ€ì „ ì „ëµ ì™„ì„±
- **5M ìŠ¤í…**: Unity ì›ë³¸ ì„±ëŠ¥ ë‹¬ì„±

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìµœì†Œ**: CPU (í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼)
- **ê¶Œì¥**: GPU (Tesla T4 ì´ìƒ)
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **ì €ì¥ê³µê°„**: 1GB (ëª¨ë¸ + ë¡œê·¸)

## ğŸ¤ ê¸°ì—¬ ë° í™•ì¥

### í™˜ê²½ í™•ì¥
```python
# ë§µ í¬ê¸° ë³€ê²½
class CustomPaperIOEnv(PaperIOEnv):
    def __init__(self):
        super().__init__()
        self.MAP_SIZE = 200  # ë” í° ë§µ
```

### ìƒˆë¡œìš´ ë³´ìƒ í•¨ìˆ˜
```python
# ì»¤ìŠ¤í…€ ë³´ìƒ ì¶”ê°€
def custom_reward_function(self, player_id, action, next_state):
    reward = 0.0
    # ì»¤ìŠ¤í…€ ë¡œì§...
    return reward
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [Unity ML-Agents ê³µì‹ ë¬¸ì„œ](https://github.com/Unity-Technologies/ml-agents)
- [PPO ì•Œê³ ë¦¬ì¦˜ ë…¼ë¬¸](https://arxiv.org/abs/1707.06347)
- [ì›ë³¸ Unity í”„ë¡œì íŠ¸](https://github.com/4sz5sz6sz/Project-AI-Paper.io-ML-Agents)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ì›ë³¸ Unity í”„ë¡œì íŠ¸ì™€ ë™ì¼í•œ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

**Paper.io ML-Agents Colab Training Environment**  
*Unity ML-Agentsë¥¼ Pythonìœ¼ë¡œ ì™„ì „ ì¬êµ¬í˜„í•œ Colab í•™ìŠµ í™˜ê²½*